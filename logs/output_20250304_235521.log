Starting script
non debug mode and not nirvana - setting local_train_data to False
2025-03-04 23:55:29.328 | INFO     | __main__:main:413 - Global rank 0, local rank 0, device: 0
2025-03-04 23:55:29.332 | INFO     | __main__:main:417 - Process group initialized
2025-03-04 23:55:29.332 | INFO     | __main__:main:433 - Using dist with rank 0 (only rank 0 will log)
2025-03-04 23:55:29.333 | INFO     | __main__:main:434 - ****************************************
2025-03-04 23:55:29.333 | INFO     | __main__:main:435 - Starting training with the arguments
2025-03-04 23:55:29.333 | INFO     | __main__:main:437 - model_config                   configs/llama_130m.json
2025-03-04 23:55:29.333 | INFO     | __main__:main:437 - use_hf_model                   False
2025-03-04 23:55:29.333 | INFO     | __main__:main:437 - batch_size                     128
2025-03-04 23:55:29.334 | INFO     | __main__:main:437 - eval_batch_size                256
2025-03-04 23:55:29.334 | INFO     | __main__:main:437 - gradient_accumulation          4
2025-03-04 23:55:29.334 | INFO     | __main__:main:437 - total_batch_size               512
2025-03-04 23:55:29.334 | INFO     | __main__:main:437 - max_length                     256
2025-03-04 23:55:29.335 | INFO     | __main__:main:437 - optimizer                      aid_sign_sgd
2025-03-04 23:55:29.335 | INFO     | __main__:main:437 - lr                             0.001
2025-03-04 23:55:29.335 | INFO     | __main__:main:437 - scheduler                      cosine
2025-03-04 23:55:29.335 | INFO     | __main__:main:437 - scheduler_cycle_length         None
2025-03-04 23:55:29.335 | INFO     | __main__:main:437 - scheduler_min_power            -20
2025-03-04 23:55:29.336 | INFO     | __main__:main:437 - min_lr_ratio                   0.1
2025-03-04 23:55:29.336 | INFO     | __main__:main:437 - activation_checkpointing       False
2025-03-04 23:55:29.336 | INFO     | __main__:main:437 - eval_every                     2000
2025-03-04 23:55:29.336 | INFO     | __main__:main:437 - num_training_steps             100000
2025-03-04 23:55:29.336 | INFO     | __main__:main:437 - max_train_tokens               None
2025-03-04 23:55:29.337 | INFO     | __main__:main:437 - save_every                     1000
2025-03-04 23:55:29.337 | INFO     | __main__:main:437 - general_save_dir               checkpoints
2025-03-04 23:55:29.337 | INFO     | __main__:main:437 - save_dir                       130m/opt-aid_sign_sgd-dtype-fp32-amp-1-bs-512-sch-cosine-warmup-10000-wd-0.001-lr-0.001-clip-1.0-l_inf-100.0-d_0-None-lb-0.0-clamp-0.001-gap-10-seed-0
2025-03-04 23:55:29.338 | INFO     | __main__:main:437 - save_dir_prefix                130m
2025-03-04 23:55:29.338 | INFO     | __main__:main:437 - tags                           None
2025-03-04 23:55:29.339 | INFO     | __main__:main:437 - dtype                          torch.float32
2025-03-04 23:55:29.339 | INFO     | __main__:main:437 - amp                            True
2025-03-04 23:55:29.339 | INFO     | __main__:main:437 - workers                        8
2025-03-04 23:55:29.339 | INFO     | __main__:main:437 - seed                           0
2025-03-04 23:55:29.339 | INFO     | __main__:main:437 - name                           test
2025-03-04 23:55:29.340 | INFO     | __main__:main:437 - grad_clipping                  1.0
2025-03-04 23:55:29.340 | INFO     | __main__:main:437 - wandb_tags                     ['aid_sign_sgd', 'fp32', 'amp', 'bs_512', 'sch_cosine', 'wd_0.001', 'lr_0.001', 'clip-1.0', 'l_inf_100.0', 'option2', 'seed_0']
2025-03-04 23:55:29.340 | INFO     | __main__:main:437 - wandb_name_prefix              None
2025-03-04 23:55:29.340 | INFO     | __main__:main:437 - run_final_eval                 True
2025-03-04 23:55:29.340 | INFO     | __main__:main:437 - run_old_eval                   False
2025-03-04 23:55:29.341 | INFO     | __main__:main:437 - final_save                     False
2025-03-04 23:55:29.341 | INFO     | __main__:main:437 - wandb                          True
2025-03-04 23:55:29.341 | INFO     | __main__:main:437 - debug                          False
2025-03-04 23:55:29.341 | INFO     | __main__:main:437 - debug_train_data               False
2025-03-04 23:55:29.341 | INFO     | __main__:main:437 - debug_print                    False
2025-03-04 23:55:29.342 | INFO     | __main__:main:437 - compile                        True
2025-03-04 23:55:29.342 | INFO     | __main__:main:437 - attn_implementation            sdpa
2025-03-04 23:55:29.342 | INFO     | __main__:main:437 - fsdp                           False
2025-03-04 23:55:29.342 | INFO     | __main__:main:437 - cpu_offload                    False
2025-03-04 23:55:29.343 | INFO     | __main__:main:437 - mp_policy_param                torch.float32
2025-03-04 23:55:29.343 | INFO     | __main__:main:437 - mp_policy_reduce               torch.float32
2025-03-04 23:55:29.344 | INFO     | __main__:main:437 - mp_policy_buffer               torch.float32
2025-03-04 23:55:29.344 | INFO     | __main__:main:437 - use_orig_params                False
2025-03-04 23:55:29.344 | INFO     | __main__:main:437 - proj_params_lr_scale           1.0
2025-03-04 23:55:29.344 | INFO     | __main__:main:437 - update_gap                     10
2025-03-04 23:55:29.344 | INFO     | __main__:main:437 - density                        0.25
2025-03-04 23:55:29.345 | INFO     | __main__:main:437 - reset_statistics               True
2025-03-04 23:55:29.345 | INFO     | __main__:main:437 - inactive_update_rule           sign_sgd
2025-03-04 23:55:29.345 | INFO     | __main__:main:437 - inactive_lr_scale              1.0
2025-03-04 23:55:29.345 | INFO     | __main__:main:437 - proj_norms                     True
2025-03-04 23:55:29.345 | INFO     | __main__:main:437 - proj_embeds                    True
2025-03-04 23:55:29.346 | INFO     | __main__:main:437 - proj_logits                    False
2025-03-04 23:55:29.346 | INFO     | __main__:main:437 - rank                           0
2025-03-04 23:55:29.346 | INFO     | __main__:main:437 - proj_side                      std
2025-03-04 23:55:29.346 | INFO     | __main__:main:437 - proj_type                      svd
2025-03-04 23:55:29.346 | INFO     | __main__:main:437 - coord_choice                   columns
2025-03-04 23:55:29.347 | INFO     | __main__:main:437 - block_order                    random
2025-03-04 23:55:29.347 | INFO     | __main__:main:437 - apollo_proj                    random
2025-03-04 23:55:29.347 | INFO     | __main__:main:437 - apollo_scale_type              tensor
2025-03-04 23:55:29.347 | INFO     | __main__:main:437 - apollo_scale                   1.0
2025-03-04 23:55:29.348 | INFO     | __main__:main:437 - apollo_scale_front             False
2025-03-04 23:55:29.348 | INFO     | __main__:main:437 - ldadam_rho                     0.908
2025-03-04 23:55:29.348 | INFO     | __main__:main:437 - ldadam_proj_method             power_iteration
2025-03-04 23:55:29.349 | INFO     | __main__:main:437 - ldadam_error_feedback          False
2025-03-04 23:55:29.349 | INFO     | __main__:main:437 - fira_alpha                     1.0
2025-03-04 23:55:29.349 | INFO     | __main__:main:437 - delay_start_step               10000
2025-03-04 23:55:29.349 | INFO     | __main__:main:437 - grad_taylor_approx             False
2025-03-04 23:55:29.349 | INFO     | __main__:main:437 - taylor_linear_coef             1.0
2025-03-04 23:55:29.349 | INFO     | __main__:main:437 - measure_time                   False
2025-03-04 23:55:29.349 | INFO     | __main__:main:437 - collect_grads                  False
2025-03-04 23:55:29.349 | INFO     | __main__:main:437 - weight_decay                   0.001
2025-03-04 23:55:29.349 | INFO     | __main__:main:437 - warmup_steps                   10000
2025-03-04 23:55:29.349 | INFO     | __main__:main:437 - beta1                          0.9
2025-03-04 23:55:29.350 | INFO     | __main__:main:437 - beta2                          0.999
2025-03-04 23:55:29.350 | INFO     | __main__:main:437 - eps                            1e-08
2025-03-04 23:55:29.350 | INFO     | __main__:main:437 - momentum                       0.0
2025-03-04 23:55:29.350 | INFO     | __main__:main:437 - nesterov                       False
2025-03-04 23:55:29.350 | INFO     | __main__:main:437 - dampening                      0
2025-03-04 23:55:29.350 | INFO     | __main__:main:437 - sgd_sign_update                False
2025-03-04 23:55:29.350 | INFO     | __main__:main:437 - sign_norm                      False
2025-03-04 23:55:29.350 | INFO     | __main__:main:437 - normalized                     False
2025-03-04 23:55:29.350 | INFO     | __main__:main:437 - l_inf                          100.0
2025-03-04 23:55:29.351 | INFO     | __main__:main:437 - d_0                            None
2025-03-04 23:55:29.351 | INFO     | __main__:main:437 - lower_bound                    0.0
2025-03-04 23:55:29.351 | INFO     | __main__:main:437 - clamp_level                    0.001
2025-03-04 23:55:29.351 | INFO     | __main__:main:437 - single_gpu                     False
2025-03-04 23:55:29.351 | INFO     | __main__:main:437 - local_train_data               False
2025-03-04 23:55:29.351 | INFO     | __main__:main:437 - streaming                      True
2025-03-04 23:55:29.351 | INFO     | __main__:main:437 - wandb_name                     opt-aid_sign_sgd-dtype-fp32-amp-1-bs-512-sch-cosine-warmup-10000-wd-0.001-lr-0.001-clip-1.0-l_inf-100.0-d_0-None-lb-0.0-clamp-0.001-gap-10-seed-0
2025-03-04 23:55:29.351 | INFO     | __main__:main:438 - ****************************************
/home/rinya/anaconda3/envs/sign-sgd-env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
DistributedDataParallel(
  (module): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 768, padding_idx=31999)
      (layers): ModuleList(
        (0-11): 12 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=768, out_features=768, bias=False)
            (k_proj): Linear(in_features=768, out_features=768, bias=False)
            (v_proj): Linear(in_features=768, out_features=768, bias=False)
            (o_proj): Linear(in_features=768, out_features=768, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=768, out_features=2048, bias=False)
            (down_proj): Linear(in_features=2048, out_features=768, bias=False)
            (up_proj): Linear(in_features=768, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=768, out_features=32000, bias=False)
  )
)
Model dtype:  torch.float32
2025-03-04 23:55:34.894 | INFO     | __main__:main:515 - ****************************************
2025-03-04 23:55:34.895 | INFO     | __main__:main:520 - Loading training state like global_step, update_step, and tokens_seen from checkpoints/130m/opt-aid_sign_sgd-dtype-fp32-amp-1-bs-512-sch-cosine-warmup-10000-wd-0.001-lr-0.001-clip-1.0-l_inf-100.0-d_0-None-lb-0.0-clamp-0.001-gap-10-seed-0
2025-03-04 23:55:34.895 | INFO     | __main__:main:528 - global_step       : 44000
2025-03-04 23:55:34.895 | INFO     | __main__:main:529 - update_step       : 11000
2025-03-04 23:55:34.896 | INFO     | __main__:main:530 - tokens_seen       : 1099940398
2025-03-04 23:55:34.896 | INFO     | __main__:main:531 - tokens_seen_before: 1099839473
2025-03-04 23:55:34.896 | INFO     | __main__:main:532 - Will train for 89000 update steps
2025-03-04 23:55:34.896 | INFO     | __main__:main:535 - ****************************************
2025-03-04 23:55:47.050 | INFO     | __main__:main:555 - Shuffling data with seed 42
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: rinya (mipt_rinya). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /home/rinya/LoRA-dev/wandb/run-20250304_235548-a8n35a9v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run opt-aid_sign_sgd-dtype-fp32-amp-1-bs-512-sch-cosine-warmup-10000-wd-0.001-lr-0.001-clip-1.0-l_inf-100.0-d_0-None-lb-0.0-clamp-0.001-gap-10-seed-0
wandb: ⭐️ View project at https://wandb.ai/mipt_rinya/8_aid_sign_sgd
wandb: 🚀 View run at https://wandb.ai/mipt_rinya/8_aid_sign_sgd/runs/a8n35a9v
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/89000 [00:00<?, ?it/s]proj_norms: True, proj_embeds: True, proj_logits: False, 
Number of proj_params: 110, number of NON proj_params: 1
2025-03-04 23:55:49.448 | INFO     | __main__:main:617 - 
DistributedDataParallel(
  (module): LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 768, padding_idx=31999)
      (layers): ModuleList(
        (0-11): 12 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=768, out_features=768, bias=False)
            (k_proj): Linear(in_features=768, out_features=768, bias=False)
            (v_proj): Linear(in_features=768, out_features=768, bias=False)
            (o_proj): Linear(in_features=768, out_features=768, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=768, out_features=2048, bias=False)
            (down_proj): Linear(in_features=2048, out_features=768, bias=False)
            (up_proj): Linear(in_features=768, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=768, out_features=32000, bias=False)
  )
)

2025-03-04 23:55:49.449 | INFO     | __main__:main:618 - Total params: 134.11M
2025-03-04 23:55:49.449 | INFO     | __main__:main:619 - Trainable params: 134.11M
2025-03-04 23:55:49.449 | INFO     | __main__:main:622 - Total params with GaLore enabled: 109.53M
2025-03-04 23:55:49.450 | INFO     | __main__:main:623 - Saving model to checkpoints/130m/opt-aid_sign_sgd-dtype-fp32-amp-1-bs-512-sch-cosine-warmup-10000-wd-0.001-lr-0.001-clip-1.0-l_inf-100.0-d_0-None-lb-0.0-clamp-0.001-gap-10-seed-0 every 1000 update steps
AIDsignSGD (
Parameter Group 0
    L_inf: 100.0
    clamp_level: 0.001
    d: None
    dampening: 0
    differentiable: False
    f_diff: None
    foreach: False
    fused: False
    is_proj_params: False
    lambda_denom_sum: None
    lower_bound: 0.0
    lr: 0.0010000000474974513
    maximize: False
    momentum: 0
    nesterov: False
    prev_gamma: None
    tilde_d: None
    update_gap: 10
    warmup_steps: 10000
    weight_decay: 0.001

Parameter Group 1
    L_inf: 100.0
    clamp_level: 0.001
    d: None
    dampening: 0
    differentiable: False
    f_diff: None
    foreach: False
    fused: False
    is_proj_params: True
    lambda_denom_sum: None
    lower_bound: 0.0
    lr: 0.0010000000474974513
    maximize: False
    momentum: 0
    nesterov: False
    prev_gamma: None
    tilde_d: None
    update_gap: 10
    warmup_steps: 10000
    weight_decay: 0.001
)
0 False 1
1 True 110
2025-03-04 23:55:49.451 | INFO     | __main__:main:649 - Loading model from checkpoints/130m/opt-aid_sign_sgd-dtype-fp32-amp-1-bs-512-sch-cosine-warmup-10000-wd-0.001-lr-0.001-clip-1.0-l_inf-100.0-d_0-None-lb-0.0-clamp-0.001-gap-10-seed-0
/home/rinya/LoRA-dev/torchrun_main.py:673: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  optimizer_scheduler_state_dict = torch.load(os.path.join(args.general_save_dir, args.save_dir, "optimizer.pt"), map_location=device)
2025-03-04 23:55:50.824 | INFO     | __main__:main:674 - Model successfully loaded (strict=True policy)
[rank0]:W0304 23:55:57.573000 2441104 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
Update steps:   0%|                       | 1/89000 [00:22<560:12:41, 22.66s/it]/home/rinya/LoRA-dev/torchrun_main.py:839: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  group["lr"] = torch.tensor(group["prev_gamma"][-1], device=device, dtype=torch.bfloat16)
Update steps:   0%|                       | 2/89000 [00:23<241:18:33,  9.76s/it]Update steps:   0%|                       | 3/89000 [00:24<138:41:39,  5.61s/it]Update steps:   0%|                        | 4/89000 [00:24<90:20:34,  3.65s/it]Update steps:   0%|                        | 5/89000 [00:25<63:40:22,  2.58s/it]Update steps:   0%|                        | 6/89000 [00:26<47:35:34,  1.93s/it]Update steps:   0%|                        | 7/89000 [00:26<37:20:44,  1.51s/it]Update steps:   0%|                        | 8/89000 [00:27<30:39:17,  1.24s/it]Update steps:   0%|                        | 9/89000 [00:28<26:13:41,  1.06s/it]Update steps:   0%|                       | 10/89000 [00:28<23:12:52,  1.06it/s]Update steps:   0%|                       | 11/89000 [00:29<21:06:55,  1.17it/s]Update steps:   0%|                       | 12/89000 [00:30<19:45:51,  1.25it/s]Update steps:   0%|                       | 13/89000 [00:30<18:49:08,  1.31it/s]Update steps:   0%|                       | 14/89000 [00:31<18:05:11,  1.37it/s]Update steps:   0%|                       | 15/89000 [00:32<17:37:13,  1.40it/s]Update steps:   0%|                       | 16/89000 [00:32<17:19:19,  1.43it/s]Update steps:   0%|                       | 17/89000 [00:33<17:02:21,  1.45it/s]Update steps:   0%|                       | 18/89000 [00:34<16:51:35,  1.47it/s]Update steps:   0%|                       | 19/89000 [00:34<16:45:28,  1.47it/s]Update steps:   0%|                       | 20/89000 [00:35<16:41:12,  1.48it/s]Update steps:   0%|                       | 21/89000 [00:36<16:37:54,  1.49it/s]Update steps:   0%|                       | 22/89000 [00:36<16:45:21,  1.48it/s]Update steps:   0%|                       | 23/89000 [00:37<16:40:39,  1.48it/s]Update steps:   0%|                       | 24/89000 [00:38<16:38:08,  1.49it/s]Update steps:   0%|                       | 25/89000 [00:38<16:35:44,  1.49it/s]Update steps:   0%|                       | 26/89000 [00:39<16:35:39,  1.49it/s]Update steps:   0%|                       | 27/89000 [00:40<16:34:21,  1.49it/s]Update steps:   0%|                       | 28/89000 [00:40<16:32:50,  1.49it/s]Update steps:   0%|                       | 29/89000 [00:41<16:28:00,  1.50it/s]Update steps:   0%|                       | 30/89000 [00:42<16:26:38,  1.50it/s]Update steps:   0%|                       | 31/89000 [00:42<16:29:38,  1.50it/s]Update steps:   0%|                       | 32/89000 [00:43<16:35:50,  1.49it/s]Update steps:   0%|                       | 33/89000 [00:44<16:33:52,  1.49it/s]Update steps:   0%|                       | 34/89000 [00:44<16:34:19,  1.49it/s]Update steps:   0%|                       | 35/89000 [00:45<16:39:28,  1.48it/s]Update steps:   0%|                       | 36/89000 [00:46<16:40:19,  1.48it/s]Update steps:   0%|                       | 37/89000 [00:46<16:38:20,  1.49it/s]Update steps:   0%|                       | 38/89000 [00:47<16:35:39,  1.49it/s]Update steps:   0%|                       | 39/89000 [00:48<16:34:23,  1.49it/s]Update steps:   0%|                       | 40/89000 [00:48<16:35:25,  1.49it/s]Update steps:   0%|                       | 41/89000 [00:49<16:36:57,  1.49it/s]Update steps:   0%|                       | 42/89000 [00:50<16:42:47,  1.48it/s]Update steps:   0%|                       | 43/89000 [00:50<16:44:00,  1.48it/s]Update steps:   0%|                       | 44/89000 [00:51<16:39:17,  1.48it/s]Update steps:   0%|                       | 45/89000 [00:52<16:38:21,  1.49it/s]Update steps:   0%|                       | 46/89000 [00:52<16:42:15,  1.48it/s]Update steps:   0%|                       | 47/89000 [00:53<16:40:20,  1.48it/s]Update steps:   0%|                       | 48/89000 [00:54<16:38:30,  1.48it/s]Update steps:   0%|                       | 49/89000 [00:54<16:37:33,  1.49it/s]Update steps:   0%|                       | 50/89000 [00:55<16:37:02,  1.49it/s]Update steps:   0%|                       | 51/89000 [00:56<16:38:50,  1.48it/s]Update steps:   0%|                       | 52/89000 [00:56<16:41:15,  1.48it/s]Update steps:   0%|                       | 53/89000 [00:57<16:42:49,  1.48it/s]Update steps:   0%|                       | 54/89000 [00:58<16:49:47,  1.47it/s]Update steps:   0%|                       | 55/89000 [00:58<16:47:13,  1.47it/s]Update steps:   0%|                       | 56/89000 [00:59<16:45:30,  1.47it/s]Update steps:   0%|                       | 57/89000 [01:00<16:43:48,  1.48it/s]Update steps:   0%|                       | 58/89000 [01:00<16:42:13,  1.48it/s]Update steps:   0%|                       | 59/89000 [01:01<16:42:09,  1.48it/s]Update steps:   0%|                       | 60/89000 [01:02<16:41:20,  1.48it/s]Update steps:   0%|                       | 61/89000 [01:02<16:43:12,  1.48it/s]Update steps:   0%|                       | 62/89000 [01:03<16:50:57,  1.47it/s]Update steps:   0%|                       | 63/89000 [01:04<16:48:31,  1.47it/s]Update steps:   0%|                       | 64/89000 [01:05<16:53:00,  1.46it/s]Update steps:   0%|                       | 65/89000 [01:05<16:50:23,  1.47it/s]Update steps:   0%|                       | 66/89000 [01:06<16:44:11,  1.48it/s]Update steps:   0%|                       | 67/89000 [01:07<16:42:26,  1.48it/s]Update steps:   0%|                       | 68/89000 [01:07<16:43:33,  1.48it/s]Update steps:   0%|                       | 69/89000 [01:08<16:43:25,  1.48it/s]Update steps:   0%|                       | 70/89000 [01:09<16:43:03,  1.48it/s]Update steps:   0%|                       | 71/89000 [01:09<16:43:06,  1.48it/s]Update steps:   0%|                       | 72/89000 [01:10<16:48:41,  1.47it/s]Update steps:   0%|                       | 73/89000 [01:11<16:50:25,  1.47it/s]Update steps:   0%|                       | 74/89000 [01:11<16:59:19,  1.45it/s]Update steps:   0%|                       | 75/89000 [01:12<16:54:54,  1.46it/s]Update steps:   0%|                       | 76/89000 [01:13<16:53:16,  1.46it/s]Update steps:   0%|                       | 77/89000 [01:13<16:52:39,  1.46it/s]Update steps:   0%|                       | 78/89000 [01:14<16:49:39,  1.47it/s]Update steps:   0%|                       | 79/89000 [01:15<16:47:30,  1.47it/s]Update steps:   0%|                       | 80/89000 [01:15<16:45:59,  1.47it/s]Update steps:   0%|                       | 81/89000 [01:16<16:46:55,  1.47it/s]Update steps:   0%|                       | 82/89000 [01:17<16:54:20,  1.46it/s]Update steps:   0%|                       | 83/89000 [01:17<16:52:05,  1.46it/s]Update steps:   0%|                       | 84/89000 [01:18<16:50:00,  1.47it/s]Update steps:   0%|                       | 85/89000 [01:19<16:48:50,  1.47it/s]Update steps:   0%|                       | 86/89000 [01:20<16:51:54,  1.46it/s]Update steps:   0%|                       | 87/89000 [01:20<16:50:43,  1.47it/s]Update steps:   0%|                       | 88/89000 [01:21<16:50:43,  1.47it/s]Update steps:   0%|                       | 89/89000 [01:22<16:50:44,  1.47it/s]Update steps:   0%|                       | 90/89000 [01:22<16:50:12,  1.47it/s]Update steps:   0%|                       | 91/89000 [01:23<16:50:06,  1.47it/s]Update steps:   0%|                       | 92/89000 [01:24<16:54:15,  1.46it/s]Update steps:   0%|                       | 93/89000 [01:24<16:52:22,  1.46it/s]Update steps:   0%|                       | 94/89000 [01:25<16:54:16,  1.46it/s]
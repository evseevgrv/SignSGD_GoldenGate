@article{veprikov2024new,
  title={New aspects of black box conditional gradient: Variance reduction and one point feedback},
  author={Veprikov, Andrey and Bogdanov, Alexander and Minashkin, Vladislav and Beznosikov, Aleksandr},
  journal={Chaos, Solitons \& Fractals},
  volume={189},
  pages={115654},
  year={2024},
  publisher={Elsevier}
}

@misc{zo_bench,
      title={Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark}, 
      author={Yihua Zhang and Pingzhi Li and Junyuan Hong and Jiaxiang Li and Yimeng Zhang and Wenqing Zheng and Pin-Yu Chen and Jason D. Lee and Wotao Yin and Mingyi Hong and Zhangyang Wang and Sijia Liu and Tianlong Chen},
      year={2024},
      eprint={2402.11592},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.11592}, 
}

@misc{muon_base,
  author       = {Keller Jordan and Yuchen Jin and Vlado Boza and Jiacheng You and
                  Franz Cesista and Laker Newhouse and Jeremy Bernstein},
  title        = {Muon: An optimizer for hidden layers in neural networks},
  year         = {2024},
  url          = {https://kellerjordan.github.io/posts/muon/}
}

@misc{muon_scaled,
      title={Muon is Scalable for LLM Training}, 
      author={Jingyuan Liu and Jianlin Su and Xingcheng Yao and Zhejun Jiang and Guokun Lai and Yulun Du and Yidao Qin and Weixin Xu and Enzhe Lu and Junjie Yan and Yanru Chen and Huabin Zheng and Yibo Liu and Shaowei Liu and Bohong Yin and Weiran He and Han Zhu and Yuzhi Wang and Jianzhou Wang and Mengnan Dong and Zheng Zhang and Yongsheng Kang and Hao Zhang and Xinran Xu and Yutao Zhang and Yuxin Wu and Xinyu Zhou and Zhilin Yang},
      year={2025},
      eprint={2502.16982},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.16982}, 
}

@misc{AdamW,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}

@misc{Shampoo,
      title={Shampoo: Preconditioned Stochastic Tensor Optimization}, 
      author={Vineet Gupta and Tomer Koren and Yoram Singer},
      year={2018},
      eprint={1802.09568},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.09568}, 
}

@misc{muon_convergence,
      title={A Note on the Convergence of Muon and Further}, 
      author={Jiaxiang Li and Mingyi Hong},
      year={2025},
      eprint={2502.02900},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2502.02900}, 
}

@misc{extreme_sparsity,
      title={Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity}, 
      author={Wentao Guo and Jikai Long and Yimeng Zeng and Zirui Liu and Xinyu Yang and Yide Ran and Jacob R. Gardner and Osbert Bastani and Christopher De Sa and Xiaodong Yu and Beidi Chen and Zhaozhuo Xu},
      year={2024},
      eprint={2406.02913},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.02913},
}

@misc{zo_random_subspaces,
      title={Zeroth-Order Fine-Tuning of LLMs in Random Subspaces}, 
      author={Ziming Yu and Pan Zhou and Sike Wang and Jia Li and Hua Huang},
      year={2024},
      eprint={2410.08989},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.08989}, 
}

@Article{lozo,
 author = {Yiming Chen and Yuan Zhang and Liyuan Cao and Kun Yuan and Zaiwen Wen},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures},
 volume = {abs/2410.07698},
 year = {2024}
}

@inproceedings{sun2023momentum,
  title={Momentum ensures convergence of signsgd under weaker assumptions},
  author={Sun, Tao and Wang, Qingsong and Li, Dongsheng and Wang, Bao},
  booktitle={International Conference on Machine Learning},
  pages={33077--33099},
  year={2023},
  organization={PMLR}
}

@article{nozawa2025zeroth,
  title={Zeroth-Order Random Subspace Algorithm for Non-smooth Convex Optimization},
  author={Nozawa, Ryota and Poirion, Pierre-Louis and Takeda, Akiko},
  journal={Journal of Optimization Theory and Applications},
  volume={204},
  number={3},
  pages={53},
  year={2025},
  publisher={Springer}
}

@article{roberts2023direct,
  title={Direct search based on probabilistic descent in reduced spaces},
  author={Roberts, Lindon and Royer, Cl{\'e}ment W},
  journal={SIAM Journal on Optimization},
  volume={33},
  number={4},
  pages={3057--3082},
  year={2023},
  publisher={SIAM}
}

@article{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{wang2024simultaneous,
  title={Simultaneous Computation and Memory Efficient Zeroth-Order Optimizer for Fine-Tuning Large Language Models},
  author={Wang, Fei and Shen, Li and Ding, Liang and Xue, Chao and Liu, Ye and Ding, Changxing},
  journal={arXiv preprint arXiv:2410.09823},
  year={2024}
}

@article{liu2024sparse,
  title={Sparse mezo: Less parameters for better performance in zeroth-order llm fine-tuning},
  author={Liu, Yong and Zhu, Zirui and Gong, Chaoyu and Cheng, Minhao and Hsieh, Cho-Jui and You, Yang},
  journal={arXiv preprint arXiv:2402.15751},
  year={2024}
}

@article{malladi2023fine,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53038--53075},
  year={2023}
}


@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{gorbunov2019stochastic,
  title={A stochastic derivative free optimization method with momentum},
  author={Gorbunov, Eduard and Bibi, Adel and Sener, Ozan and Bergou, El Houcine and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1905.13278},
  year={2019}
}

@article{gorbunov2022accelerated,
  title={An accelerated method for derivative-free smooth stochastic convex optimization},
  author={Gorbunov, Eduard and Dvurechensky, Pavel and Gasnikov, Alexander},
  journal={SIAM Journal on Optimization},
  volume={32},
  number={2},
  pages={1210--1238},
  year={2022},
  publisher={SIAM}
}

@article{rando2024stochastic,
  title={Stochastic zeroth order descent with structured directions},
  author={Rando, Marco and Molinari, Cesare and Villa, Silvia and Rosasco, Lorenzo},
  journal={Computational Optimization and Applications},
  pages={1--37},
  year={2024},
  publisher={Springer}
}

@article{dvurechensky2021accelerated,
  title={An accelerated directional derivative method for smooth stochastic convex optimization},
  author={Dvurechensky, Pavel and Gorbunov, Eduard and Gasnikov, Alexander},
  journal={European Journal of Operational Research},
  volume={290},
  number={2},
  pages={601--621},
  year={2021},
  publisher={Elsevier}
}

@article{bernstein2024old,
  title={Old optimizer, new norm: An anthology},
  author={Bernstein, Jeremy and Newhouse, Laker},
  journal={arXiv preprint arXiv:2409.20325},
  year={2024}
}

@article{bernstein2024modular,
  title={Modular duality in deep learning},
  author={Bernstein, Jeremy and Newhouse, Laker},
  journal={arXiv preprint arXiv:2410.21265},
  year={2024}
}

@article{pethick2025training,
  title={Training Deep Learning Models with Norm-Constrained LMOs},
  author={Pethick, Thomas and Xie, Wanyun and Antonakopoulos, Kimon and Zhu, Zhenyu and Silveti-Falls, Antonio and Cevher, Volkan},
  journal={arXiv preprint arXiv:2502.07529},
  year={2025}
}

@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}

@article{vyas2024soap,
  title={Soap: Improving and stabilizing shampoo using adam},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Kwun, Mujin and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}

@article{dahl2023benchmarking,
  title={Benchmarking neural network training algorithms},
  author={Dahl, George E and Schneider, Frank and Nado, Zachary and Agarwal, Naman and Sastry, Chandramouli Shama and Hennig, Philipp and Medapati, Sourabh and Eschenhagen, Runa and Kasimbeg, Priya and Suo, Daniel and others},
  journal={arXiv preprint arXiv:2306.07179},
  year={2023}
}

@article{lian2016comprehensive,
  title={A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order},
  author={Lian, Xiangru and Zhang, Huan and Hsieh, Cho-Jui and Huang, Yijun and Liu, Ji},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{kornilov2025sign,
  title={Sign Operator for Coping with Heavy-Tailed Noise: High Probability Convergence Bounds with Extensions to Distributed Optimization and Comparison Oracle},
  author={Kornilov, Nikita and Zmushko, Philip and Semenov, Andrei and Gasnikov, Alexander and Beznosikov, Alexander},
  journal={arXiv preprint arXiv:2502.07923},
  year={2025}
}

@article{nesterov2012efficiency,
  title={Efficiency of coordinate descent methods on huge-scale optimization problems},
  author={Nesterov, Yu},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={2},
  pages={341--362},
  year={2012},
  publisher={SIAM}
}

@article{richtarik2016distributed,
  title={Distributed coordinate descent method for learning with big data},
  author={Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={75},
  pages={1--25},
  year={2016}
}

@inproceedings{liu2019signsgd,
  title={signSGD via zeroth-order oracle},
  author={Liu, Sijia and Chen, Pin-Yu and Chen, Xiangyi and Hong, Mingyi},
  booktitle={International conference on learning representations},
  year={2019}
}

@article{bernstein2018signsgd,
  title={signSGD with majority vote is communication efficient and fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1810.05291},
  year={2018}
}

@inproceedings{safaryan2021stochastic,
  title={Stochastic sign descent methods: New algorithms and better theory},
  author={Safaryan, Mher and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={9224--9234},
  year={2021},
  organization={PMLR}
}

@article{jin2020stochastic,
  title={Stochastic-sign SGD for federated learning with theoretical guarantees},
  author={Jin, Richeng and Huang, Yufan and He, Xiaofan and Dai, Huaiyu and Wu, Tianfu},
  journal={arXiv preprint arXiv:2002.10940},
  year={2020}
}

@article{huang2022accelerated,
  title={Accelerated zeroth-order and first-order momentum methods from mini to minimax optimization},
  author={Huang, Feihu and Gao, Shangqian and Pei, Jian and Huang, Heng},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={36},
  pages={1--70},
  year={2022}
}

@article{chen2019zo,
  title={Zo-adamm: Zeroth-order adaptive momentum method for black-box optimization},
  author={Chen, Xiangyi and Liu, Sijia and Xu, Kaidi and Li, Xingguo and Lin, Xue and Hong, Mingyi and Cox, David},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{qian2023zeroth,
  title={Zeroth-Order Proximal Stochastic Recursive Momentum Algorithm for Nonconvex Nonsmooth Optimization},
  author={Qian, Yuxiang and Zhao, Yong},
  booktitle={2023 International Conference on New Trends in Computational Intelligence (NTCI)},
  volume={1},
  pages={419--423},
  year={2023},
  organization={IEEE}
}

@inproceedings{reddy2023convergence,
  title={Convergence of momentum-based heavy ball method with batch updating and/or approximate gradients},
  author={Reddy, Tadipatri Uday Kiran and Vidyasagar, Mathukumalli},
  booktitle={2023 Ninth Indian Control Conference (ICC)},
  pages={182--187},
  year={2023},
  organization={IEEE}
}

@inproceedings{jiang2024zo,
  title={Zo-adamu optimizer: Adapting perturbation by the momentum and uncertainty in zeroth-order optimization},
  author={Jiang, Shuoran and Chen, Qingcai and Pan, Youcheng and Xiang, Yang and Lin, Yukang and Wu, Xiangping and Liu, Chuanyi and Song, Xiaobao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={18363--18371},
  year={2024}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM journal on optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{ghadimi2016mini,
  title={Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization},
  author={Ghadimi, Saeed and Lan, Guanghui and Zhang, Hongchao},
  journal={Mathematical Programming},
  volume={155},
  number={1},
  pages={267--305},
  year={2016},
  publisher={Springer}
}

@article{guo2024zeroth,
  title={Zeroth-order fine-tuning of llms with extreme sparsity},
  author={Guo, Wentao and Long, Jikai and Zeng, Yimeng and Liu, Zirui and Yang, Xinyu and Ran, Yide and Gardner, Jacob R and Bastani, Osbert and De Sa, Christopher and Yu, Xiaodong and others},
  journal={arXiv preprint arXiv:2406.02913},
  year={2024}
}

@article{maass2021zeroth,
  title={Zeroth-order optimization on subsets of symmetric matrices with application to mpc tuning},
  author={Maass, Alejandro I and Manzie, Chris and Shames, Iman and Nakada, Hayato},
  journal={IEEE Transactions on Control Systems Technology},
  volume={30},
  number={4},
  pages={1654--1667},
  year={2021},
  publisher={IEEE}
}

@misk{raffel2023exploringlimitstransferlearning,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@misk{sanh2022multitaskpromptedtrainingenables,
      title={Multitask Prompted Training Enables Zero-Shot Task Generalization}, 
      author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Tali Bers and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M. Rush},
      year={2022},
      eprint={2110.08207},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.08207}, 
}

@article{amari1993backpropagation,
title={Backpropagation and Stochastic Gradient Descent Method},
author={Amari, Shun-ichi},
journal={Neurocomputing},
volume={5},
number={4},
pages={185--196},
year={1993}}

@article{kingma2014adam,
title={Adam: A Method for Stochastic Optimization},
author={Kingma, Diederik P. and Ba, Jimmy},
journal={arXiv preprint},
volume={abs/1412.6980},
year={2014},
eprint={1412.6980},
archivePrefix={arXiv},
primaryClass={cs.LG}}

@article{zhu2023efficient,
title={Efficient Fine-Tuning of Language Models via Zeroth-Order Optimization},
author={Zhu, Yujia and Zhang, Yujing and Zhang, Ziwei and others},
journal={arXiv preprint},
volume={abs/2305.14395},
year={2023},
eprint={2305.14395},
archivePrefix={arXiv},
primaryClass={cs.LG}}

@article{han2015deep,
title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
author={Han, Song and Mao, Huizi and Dally, William J.},
journal={arXiv preprint},
volume={abs/1510.00149},
year={2015},
eprint={1510.00149},
archivePrefix={arXiv},
primaryClass={cs.CV}}

@article{malladi2023mezo,
title={MeZO: Memory-Efficient Zeroth-Order Optimization of Large Language Models},
author={Malladi, Bharath and Aghazadeh, Amir and Tang, Haotian and others},
journal={arXiv preprint},
volume={abs/2305.18660},
year={2023},
eprint={2305.18660},
archivePrefix={arXiv},
primaryClass={cs.LG}}

@inproceedings{balles2017dissecting,
title={Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients},
author={Balles, Lukas and Hennig, Philipp},
booktitle={International Conference on Machine Learning (ICML)},
year={2017},
eprint={1705.07774},
archivePrefix={arXiv},
primaryClass={cs.LG}}

@article{shrivastava2015communication,
title={Communication-efficient Distributed Optimization using Compressed Gradients},
author={Shrivastava, Anshumali and Li, Ping and others},
journal={arXiv preprint},
volume={abs/1503.06889},
year={2015},
eprint={1503.06889},
archivePrefix={arXiv},
primaryClass={cs.DC}}

@inproceedings{goodfellow2015explaining,
title={Explaining and Harnessing Adversarial Examples},
author={Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
booktitle={International Conference on Learning Representations (ICLR)},
year={2015},
eprint={1412.6572},
archivePrefix={arXiv},
primaryClass={cs.CR}}

@inproceedings{madry2018towards,
title={Towards Deep Learning Models Resistant to Adversarial Attacks},
author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
booktitle={International Conference on Learning Representations (ICLR)},
year={2018},
eprint={1706.06083},
archivePrefix={arXiv},
primaryClass={stat.ML}}

@article{bernstein2024modular,
title={Modular Preconditioning for Efficient and Generalizable Deep Learning},
author={Bernstein, Jeremy and Zhang, Ruitong and Wang, Yu-Xiang},
journal={arXiv preprint},
volume={abs/2401.01822},
year={2024},
eprint={2401.01822},
archivePrefix={arXiv},
primaryClass={cs.LG}}

@article{pethick2025training,
title={Training Foundation Models with Matrix-Based Optimization},
author={Pethick, Jon and others},
note={To appear},
year={2025}}

@inproceedings{gupta2018shampoo,
title={Shampoo: Preconditioned Stochastic Tensor Optimization},
author={Gupta, Vineet and Agarwal, Badih and Reynolds, Matthew and others},
booktitle={International Conference on Machine Learning (ICML)},
year={2018},
eprint={1802.09568},
archivePrefix={arXiv},
primaryClass={cs.LG}}

@article{vyas2024soap,
title={SOAP: Second-Order Adaptive Optimizer with Projection for Training Large Models},
author={Vyas, Aashish and others},
journal={arXiv preprint},
volume={abs/2402.00001},
year={2024},
eprint={2402.00001},
archivePrefix={arXiv},
primaryClass={cs.LG}}

@article{dahl2023benchmarking,
title={Benchmarking Optimizers for Large Language Models},
author={Dahl, George and others},
journal={arXiv preprint},
volume={abs/2309.00001},
year={2023},
eprint={2309.00001},
archivePrefix={arXiv},
primaryClass={cs.LG}}

@article{muon_base,
title={Muon: Efficient Matrix-Based Optimization for Foundation Models},
author={Firstname Author1 and Firstname Author2},
journal={arXiv preprint},
volume={abs/2403.10001},
year={2024},
eprint={2403.10001},
archivePrefix={arXiv},
primaryClass={cs.LG}}

@article{muon_scaled,
title={Scaled Muon: Large-Scale Matrix Optimization Without Preconditioners},
author={Firstname Author3 and Firstname Author4},
journal={arXiv preprint},
volume={abs/2403.10002},
year={2024},
eprint={2403.10002},
archivePrefix={arXiv},
primaryClass={cs.LG}}


@inproceedings{flaxman2005online,
title={Online convex optimization in the bandit setting: gradient descent without a gradient},
author={Flaxman, Abraham D. and Kalai, Adam Tauman and McMahan, H. Brendan},
booktitle={Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)},
pages={385--394},
year={2005}}

@article{nesterov2017random,
title={Random Gradient-Free Minimization of Convex Functions},
author={Nesterov, Yurii and Spokoiny, Vladimir},
journal={Foundations of Computational Mathematics},
volume={17},
number={2},
pages={527--566},
year={2017},
doi={10.1007/s10208-015-9296-2}}

@article{duchi2015optimal,
title={Optimal Rates for Zero-Order Convex Optimization: The Power of Two Function Evaluations},
author={Duchi, John C. and Jordan, Michael I. and Wainwright, Martin J. and Wibisono, Andre},
journal={IEEE Transactions on Information Theory},
volume={61},
number={5},
pages={2788--2806},
year={2015},
doi={10.1109/TIT.2015.2413811}}

@inproceedings{flaxman2005online,
  title={Online convex optimization in the bandit setting: gradient descent without a gradient},
  author={Flaxman, Abraham D. and Kalai, Adam Tauman and McMahan, H. Brendan},
  booktitle={SODA},
  pages={385--394},
  year={2005}
}

@article{duchi2015optimal,
  title={Optimal Rates for Zero-Order Convex Optimization: The Power of Two Function Evaluations},
  author={Duchi, John C. and Jordan, Michael I. and Wainwright, Martin J. and Wibisono, Andre},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={5},
  pages={2788--2806},
  year={2015}
}
@inproceedings{shamir2013complexity,
  title={On the Complexity of Bandit and Derivative-Free Stochastic Convex Optimization},
  author={Shamir, Ohad},
  booktitle={ICML},
  pages={1001--1009},
  year={2013}
}
@article{kozlak2021zeroth,
  title={Zeroth order optimization with orthogonal random directions},
  author={Kozak, David and Molinari, Cesare and Rosasco, Lorenzo and Tenorio, Luis and Villa, Silvia},
  journal={arXiv preprint},
  volume={abs/2107.03941},
  year={2021}
}
@inproceedings{cai2021zeroth,
  title={A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization},
  author={Cai, HanQin and Lou, Yuchen and McKenzie, Daniel and Yin, Wotao},
  booktitle={ICML},
  pages={1182--1191},
  year={2021}
}
@inproceedings{zhai24a,
  title={Learning Sampling Policy to Achieve Fewer Queries for Zeroth-Order Optimization},
  author={Zhai, Zhou and Shi, Wanli and Huang, Heng and Chang, Yi and Gu, Bin},
  booktitle={AISTATS},
  pages={1162--1170},
  year={2024}
}
@article{chen2019zo_adamm,
  title={ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization},
  author={Chen, Xiangyi and Liu, Sijia and Xu, Kaidi and Li, Xingguo and Lin, Xue and Hong, Mingyi and Cox, David},
  journal={arXiv preprint arXiv:1910.06513},
  year={2019}
}

@article{openreview2025mezo_a3dam,
  title={MeZO-A$^3$dam: Memory-efficient Zeroth-order Adam with Adaptivity Adjustments},
  author={Anonymous Author et al.},
  journal={OpenReview, ICLR 2025},
  year={2024},
  note={Under review},
  url={https://openreview.net/forum?id=OBIuFjZzmp}
}

@article{duchi2015optimal,
  title={Optimal Rates for Zero-Order Convex Optimization: The Power of Two Function Evaluations},
  author={Duchi, John C. and Jordan, Michael I. and Wainwright, Martin J. and Wibisono, Andre},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={5},
  pages={2788--2806},
  year={2015}
}

@article{ghadimi2016accelerated,
  title={Accelerated Gradient Methods for Nonconvex Optimization},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={arXiv preprint arXiv:1608.06860},
  year={2016}
}

@article{socher2013recursive,
  title={Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew Y. and Potts, Christopher},
  journal={Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages={1631--1642},
  year={2013},
  publisher={Association for Computational Linguistics}
}

@misc{zhang2022opt,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Dewan, Shuohui and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and others},
  year={2022},
  eprint={2205.01068},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2205.01068}
}

@misc{liu2019roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year={2019},
  eprint={1907.11692},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/1907.11692}
}

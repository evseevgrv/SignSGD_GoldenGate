{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/mnt/LLM/hub/models--meta-llama--Meta-Llama-3-70B/blobs/fbc173eeb6cb16d1415f19e2f6590e1a1bfe55a483ae8c66e8e6ce95215f21b5'\n",
    "with safe_open(file_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    # Get tensor names in this file\n",
    "    tensor_names = f.keys()\n",
    "    # Load just one tensor\n",
    "    single_tensor = f.get_tensor(list(tensor_names)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0098, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_tensor.abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.162277660168379e-06"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 ** 2 / (100_000_000_000)) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.4506e-09)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor(1e-6, dtype=torch.bfloat16) + 1e-8).float() - (torch.tensor(1e-6, dtype=torch.bfloat16)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmushko-fa/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaConfig, AutoModel, AutoModelForCausalLM\n",
    "import json\n",
    "\n",
    "# Method 1\n",
    "with open('configs/llama_60m.json', 'r') as file:\n",
    "    config_dict = json.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "args = inspect.signature(LlamaConfig.__init__).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self\n",
      "num_key_value_heads\n",
      "max_position_embeddings\n",
      "pretraining_tp\n",
      "tie_word_embeddings\n",
      "rope_theta\n",
      "rope_scaling\n",
      "attention_bias\n",
      "attention_dropout\n",
      "mlp_bias\n",
      "head_dim\n",
      "kwargs\n"
     ]
    }
   ],
   "source": [
    "args_list = list(arg for arg in args)\n",
    "for arg in args:\n",
    "    if arg not in config_dict.keys():\n",
    "        print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architectures ['LLaMAForCausalLM']\n",
      "max_sequence_length 1024\n",
      "model_type llama\n",
      "transformers_version 4.28.1\n"
     ]
    }
   ],
   "source": [
    "args_list = list(arg for arg in args)\n",
    "new_config_dict = {}\n",
    "for k in config_dict:\n",
    "    if k not in args_list:\n",
    "        print(k, config_dict[k])\n",
    "    else:\n",
    "        new_config_dict[k] = config_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmushko-fa/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
      "  warnings.warn(\n",
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architectures model: None model auto: ['LLaMAForCausalLM']\n",
      "_name_or_path model:  model auto: configs/llama_60m.json\n",
      "transformers_version model: None model auto: 4.28.1\n"
     ]
    }
   ],
   "source": [
    "from utils.modeling_llama import LlamaForCausalLM as CustomLlama\n",
    "from transformers import AutoConfig\n",
    "\n",
    "\n",
    "config = LlamaConfig(**new_config_dict, num_key_value_heads=new_config_dict[\"num_attention_heads\"])\n",
    "torch.manual_seed(0)\n",
    "model = AutoModelForCausalLM.from_config(config).to(device)\n",
    "\n",
    "auto_config = AutoConfig.from_pretrained(\"configs/llama_60m.json\")\n",
    "torch.manual_seed(0)\n",
    "model_auto_config = AutoModelForCausalLM.from_config(auto_config).to(device)\n",
    "\n",
    "for arg in model.config:\n",
    "    if getattr(model.config, arg) != getattr(model_auto_config.config, arg):\n",
    "        print(arg, \"model:\", getattr(model.config, arg), \"model auto:\", getattr(model_auto_config.config, arg))\n",
    "\n",
    "torch.manual_seed(0)\n",
    "custom_model = CustomLlama(auto_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdpa'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_config.config._attn_implementation_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name1, p1), (name2, p2), (name3, p3) in zip(model_auto_config.named_parameters(), model.named_parameters(), custom_model.named_parameters()):\n",
    "    assert name1 == name2 == name3\n",
    "    assert p1.data.eq(p2.data).all() and p1.data.eq(p3.data).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(False, device='cuda:1'),\n",
       " tensor(True, device='cuda:1'),\n",
       " tensor(False, device='cuda:1'))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "input_ids = torch.randint(0, 32000, (2, 10), dtype=torch.long, device=device)\n",
    "logits = model(input_ids).logits\n",
    "logits2 = model(input_ids).logits\n",
    "logits_custom = custom_model(input_ids).logits\n",
    "logits_auto_config = model_auto_config(input_ids).logits\n",
    "\n",
    "logits.eq(logits_custom).all(), logits.eq(logits_auto_config).all(), logits_custom.eq(logits_auto_config).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9407e-07, device='cuda:1', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(logits - logits_c).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1785, -0.4982,  0.7107,  ...,  0.4550,  0.0583,  0.1518],\n",
       "         [-0.1425, -0.4003,  0.9012,  ...,  0.1133,  0.2125,  0.1764],\n",
       "         [-0.0111, -0.3142,  0.6533,  ...,  0.0941,  0.3157,  0.2288],\n",
       "         ...,\n",
       "         [-0.1489, -0.4574,  0.5064,  ..., -0.1208,  0.3462,  0.2093],\n",
       "         [-0.1052, -0.7117,  0.4115,  ...,  0.0022,  0.4919,  0.3694],\n",
       "         [ 0.0518, -0.8056,  0.4821,  ..., -0.1090,  0.5137,  0.3687]],\n",
       "\n",
       "        [[ 0.3210,  0.2530,  0.5245,  ...,  0.2575,  0.0839,  0.2333],\n",
       "         [ 0.4388,  0.3758,  0.5472,  ...,  0.0641, -0.3066,  0.0449],\n",
       "         [ 0.1868,  0.1871,  0.4683,  ...,  0.1585, -0.7356,  0.1738],\n",
       "         ...,\n",
       "         [ 0.5923,  0.1250,  0.3390,  ...,  0.0136, -0.7442,  0.1801],\n",
       "         [ 0.5866,  0.1197,  0.2293,  ...,  0.1272, -0.6348,  0.2422],\n",
       "         [ 0.6541,  0.2405,  0.1744,  ...,  0.0930, -0.6245,  0.1853]]],\n",
       "       device='cuda:1', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = model.model.layers[0].input_layernorm(hidden_states)\n",
    "out = model.model.layers[0].self_attn(hidden_states=hs, attention_mask=attention_mask, position_ids=position_ids, output_attentions=output_attentions)\n",
    "out_c = custom_model.model.layers[0].self_attn(hidden_states=hs, attention_mask=attention_mask, position_ids=position_ids, output_attentions=output_attentions)\n",
    "# out = model.model.layers[0].mlp(hs)\n",
    "# out_c = custom_model.model.layers[0].mlp(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True, False,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False,  True, False],\n",
       "          [False,  True,  True,  ..., False, False, False]],\n",
       " \n",
       "         [[ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ..., False,  True, False],\n",
       "          [ True,  True, False,  ...,  True, False,  True],\n",
       "          ...,\n",
       "          [False, False,  True,  ..., False, False,  True],\n",
       "          [ True, False,  True,  ...,  True, False, False],\n",
       "          [ True, False, False,  ..., False, False, False]]], device='cuda:1'),\n",
       " tensor(False, device='cuda:1'),\n",
       " False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].eq(out_c[0]), out[0].eq(out_c[0]).all(), out[0].allclose(out_c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.model.layers[0].mlp(hs)\n",
    "out_c = custom_model.model.layers[0].mlp(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp, mlp_c = model.model.layers[0].mlp, custom_model.model.layers[0].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiLU()"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.act_fn, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True, device='cuda:1'),\n",
       " tensor(False, device='cuda:1'),\n",
       " tensor(False, device='cuda:1'))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.gate_proj.weight.eq(mlp_c.gate_proj.weight).all(), mlp.up_proj.weight.eq(mlp_c.up_proj.weight).all(), mlp.down_proj.weight.eq(mlp_c.down_proj.weight).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "model model\n",
      "isinstance(child_module, torch.nn.ModuleList) False\n",
      "model\n",
      "embed_tokens embed_tokens\n",
      "isinstance(child_module, torch.nn.ModuleList) False\n",
      "tensor([[[-2.1211e-02,  1.5187e-02,  7.8467e-03,  ...,  7.0287e-04,\n",
      "          -4.3865e-02,  1.3202e-02],\n",
      "         [-1.1476e-02,  3.0487e-02, -3.3294e-02,  ...,  6.2483e-03,\n",
      "          -1.9375e-02, -5.4145e-03],\n",
      "         [ 4.3534e-03, -6.7081e-03, -1.1448e-02,  ..., -5.8181e-03,\n",
      "           2.4763e-02, -2.1937e-02],\n",
      "         ...,\n",
      "         [-5.0058e-04,  1.4783e-02, -5.9794e-03,  ...,  2.5936e-02,\n",
      "          -9.3622e-03, -6.1800e-03],\n",
      "         [ 9.7384e-04,  2.2270e-02, -1.5616e-02,  ..., -1.6225e-02,\n",
      "           1.2520e-02, -8.5498e-03],\n",
      "         [-1.1799e-02,  8.0785e-05,  5.6678e-03,  ...,  2.1458e-02,\n",
      "           2.2168e-03,  1.9806e-02]],\n",
      "\n",
      "        [[-2.9679e-02, -1.7686e-02,  8.6890e-04,  ..., -2.4632e-02,\n",
      "           3.5928e-03, -1.0772e-02],\n",
      "         [-1.3895e-02, -1.0822e-02,  2.5041e-02,  ...,  1.0523e-02,\n",
      "          -2.1157e-02,  2.4471e-02],\n",
      "         [-1.1753e-02, -4.8296e-02,  1.0680e-02,  ...,  2.9523e-03,\n",
      "           7.6858e-04, -4.6695e-02],\n",
      "         ...,\n",
      "         [ 1.7096e-02,  2.5860e-02, -3.9601e-03,  ...,  2.4944e-02,\n",
      "          -3.7871e-02, -2.3628e-02],\n",
      "         [ 1.9839e-02, -3.1680e-02, -9.5768e-03,  ..., -5.2999e-03,\n",
      "           3.4541e-02,  1.4166e-02],\n",
      "         [-7.0744e-03, -1.0255e-02,  9.5129e-04,  ..., -3.9433e-03,\n",
      "           2.7101e-02, -1.8506e-03]]], device='cuda:1',\n",
      "       grad_fn=<EmbeddingBackward0>) tensor([[[-2.1211e-02,  1.5187e-02,  7.8467e-03,  ...,  7.0287e-04,\n",
      "          -4.3865e-02,  1.3202e-02],\n",
      "         [-1.1476e-02,  3.0487e-02, -3.3294e-02,  ...,  6.2483e-03,\n",
      "          -1.9375e-02, -5.4145e-03],\n",
      "         [ 4.3534e-03, -6.7081e-03, -1.1448e-02,  ..., -5.8181e-03,\n",
      "           2.4763e-02, -2.1937e-02],\n",
      "         ...,\n",
      "         [-5.0058e-04,  1.4783e-02, -5.9794e-03,  ...,  2.5936e-02,\n",
      "          -9.3622e-03, -6.1800e-03],\n",
      "         [ 9.7384e-04,  2.2270e-02, -1.5616e-02,  ..., -1.6225e-02,\n",
      "           1.2520e-02, -8.5498e-03],\n",
      "         [-1.1799e-02,  8.0785e-05,  5.6678e-03,  ...,  2.1458e-02,\n",
      "           2.2168e-03,  1.9806e-02]],\n",
      "\n",
      "        [[-2.9679e-02, -1.7686e-02,  8.6890e-04,  ..., -2.4632e-02,\n",
      "           3.5928e-03, -1.0772e-02],\n",
      "         [-1.3895e-02, -1.0822e-02,  2.5041e-02,  ...,  1.0523e-02,\n",
      "          -2.1157e-02,  2.4471e-02],\n",
      "         [-1.1753e-02, -4.8296e-02,  1.0680e-02,  ...,  2.9523e-03,\n",
      "           7.6858e-04, -4.6695e-02],\n",
      "         ...,\n",
      "         [ 1.7096e-02,  2.5860e-02, -3.9601e-03,  ...,  2.4944e-02,\n",
      "          -3.7871e-02, -2.3628e-02],\n",
      "         [ 1.9839e-02, -3.1680e-02, -9.5768e-03,  ..., -5.2999e-03,\n",
      "           3.4541e-02,  1.4166e-02],\n",
      "         [-7.0744e-03, -1.0255e-02,  9.5129e-04,  ..., -3.9433e-03,\n",
      "           2.7101e-02, -1.8506e-03]]], device='cuda:1',\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "layers layers\n",
      "isinstance(child_module, torch.nn.ModuleList) True\n",
      "0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x, x_c)\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28minput\u001b[39m, input_c \u001b[38;5;241m=\u001b[39m x, x_c\n\u001b[0;32m---> 42\u001b[0m \u001b[43mfoo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 37\u001b[0m, in \u001b[0;36mfoo\u001b[0;34m(parent_name, parent_module, parent_name_c, parent_module_c, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m     check_c \u001b[38;5;241m=\u001b[39m check_c[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check\u001b[38;5;241m.\u001b[39meq(check_c)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mfoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_name_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_module_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(x, x_c)\n",
      "Cell \u001b[0;32mIn[75], line 20\u001b[0m, in \u001b[0;36mfoo\u001b[0;34m(parent_name, parent_module, parent_name_c, parent_module_c, input)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (layer, layer_c) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(child_module, child_module_c)):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     x_c \u001b[38;5;241m=\u001b[39m layer_c(input_c)\n\u001b[1;32m     22\u001b[0m     check, check_c \u001b[38;5;241m=\u001b[39m x, x_c\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:640\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:538\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe attention layers in this model are transitioning from computing the RoPE embeddings internally \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthrough `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremoved and `position_embeddings` will be mandatory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m     )\n\u001b[0;32m--> 538\u001b[0m     cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153\u001b[0m, in \u001b[0;36mLlamaRotaryEmbedding.forward\u001b[0;34m(self, x, position_ids)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_frequency_update(position_ids, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Core RoPE block\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m inv_freq_expanded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_freq[\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mexpand(\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    154\u001b[0m position_ids_expanded \u001b[38;5;241m=\u001b[39m position_ids[:, \u001b[38;5;28;01mNone\u001b[39;00m, :]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Force float32 (see https://github.com/huggingface/transformers/pull/29285)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def foo(parent_name, parent_module, parent_name_c, parent_module_c, input):\n",
    "    print(parent_name)\n",
    "    assert parent_name == parent_name_c, (name, name_c)\n",
    "    input = input.clone()\n",
    "    input_c = input.clone()\n",
    "    for (child_name, child_module), (child_name_c, child_module_c) in zip(parent_module.named_children(), parent_module_c.named_children()):\n",
    "        print(child_name, child_name_c)\n",
    "        if child_name != child_name_c:\n",
    "            foo(child_name, child_module, child_name_c, child_module_c, input)\n",
    "        \n",
    "        print(\"isinstance(child_module, torch.nn.ModuleList)\", isinstance(child_module, torch.nn.ModuleList))\n",
    "        if not isinstance(child_module, torch.nn.ModuleList):\n",
    "            x = child_module(input)\n",
    "            x_c = child_module_c(input_c)\n",
    "        else:\n",
    "            for i, (layer, layer_c) in enumerate(zip(child_module, child_module_c)):\n",
    "                print(i)\n",
    "                x = layer(input)\n",
    "                x_c = layer_c(input_c)\n",
    "                check, check_c = x, x_c\n",
    "                if not isinstance(check, torch.Tensor):\n",
    "                    check = check[0].clone()\n",
    "                while not isinstance(check_c, torch.Tensor):\n",
    "                    check_c = check_c[0].clone()\n",
    "                if not check.eq(check_c).all():\n",
    "                    foo(f\"layer_{i}\", layer, f\"layer_{i}\", layer_c, input)\n",
    "                    return\n",
    "                input, input_c = x, x_c\n",
    "        check, check_c = x, x_c\n",
    "        if not isinstance(check, torch.Tensor):\n",
    "            check = check[0].clone()\n",
    "        while not isinstance(check_c, torch.Tensor):\n",
    "            check_c = check_c[0].clone()\n",
    "        if not check.eq(check_c).all():\n",
    "            foo(child_name, child_module, child_name_c, child_module_c, input)\n",
    "            return\n",
    "        print(x, x_c)\n",
    "        input, input_c = x, x_c\n",
    "\n",
    "foo(\"model\", model.to(\"cuda:1\"), \"model\", custom_model.to(\"cuda:1\"), input_ids.to(\"cuda:1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parent model\n",
      "model model\n",
      "lm_head lm_head\n",
      "\n",
      "parent model\n",
      "embed_tokens embed_tokens\n",
      "layers layers\n",
      "norm norm\n",
      "\n",
      "parent embed_tokens\n",
      "\n",
      "parent layers\n",
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n",
      "5 5\n",
      "6 6\n",
      "7 7\n",
      "\n",
      "parent 0\n",
      "self_attn self_attn\n",
      "mlp mlp\n",
      "input_layernorm input_layernorm\n",
      "post_attention_layernorm post_attention_layernorm\n",
      "\n",
      "parent self_attn\n",
      "q_proj q_proj\n",
      "k_proj k_proj\n",
      "v_proj v_proj\n",
      "o_proj o_proj\n",
      "rotary_emb rotary_emb\n",
      "\n",
      "parent q_proj\n",
      "\n",
      "parent k_proj\n",
      "\n",
      "parent v_proj\n",
      "\n",
      "parent o_proj\n",
      "\n",
      "parent rotary_emb\n",
      "\n",
      "parent mlp\n",
      "gate_proj gate_proj\n",
      "up_proj down_proj\n",
      "down_proj up_proj\n",
      "act_fn act_fn\n",
      "\n",
      "parent gate_proj\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "('up_proj', 'down_proj')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (child_name, child_module), (child_name_c, child_module_c) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parent_module\u001b[38;5;241m.\u001b[39mnamed_children(), parent_module_c\u001b[38;5;241m.\u001b[39mnamed_children()):\n\u001b[1;32m      9\u001b[0m         foo(child_name, child_module, child_name_c, child_module_c)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mfoo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 9\u001b[0m, in \u001b[0;36mfoo\u001b[0;34m(parent_name, parent_module, parent_name_c, parent_module_c)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child_name, child_name_c)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (child_name, child_module), (child_name_c, child_module_c) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parent_module\u001b[38;5;241m.\u001b[39mnamed_children(), parent_module_c\u001b[38;5;241m.\u001b[39mnamed_children()):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mfoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_name_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_module_c\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 9\u001b[0m, in \u001b[0;36mfoo\u001b[0;34m(parent_name, parent_module, parent_name_c, parent_module_c)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child_name, child_name_c)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (child_name, child_module), (child_name_c, child_module_c) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parent_module\u001b[38;5;241m.\u001b[39mnamed_children(), parent_module_c\u001b[38;5;241m.\u001b[39mnamed_children()):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mfoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_name_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_module_c\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: foo at line 9 (2 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[79], line 9\u001b[0m, in \u001b[0;36mfoo\u001b[0;34m(parent_name, parent_module, parent_name_c, parent_module_c)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child_name, child_name_c)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (child_name, child_module), (child_name_c, child_module_c) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parent_module\u001b[38;5;241m.\u001b[39mnamed_children(), parent_module_c\u001b[38;5;241m.\u001b[39mnamed_children()):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mfoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_name_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_module_c\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 4\u001b[0m, in \u001b[0;36mfoo\u001b[0;34m(parent_name, parent_module, parent_name_c, parent_module_c)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfoo\u001b[39m(parent_name, parent_module, parent_name_c, parent_module_c):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m parent_name \u001b[38;5;241m==\u001b[39m parent_name_c, (parent_name, parent_name_c)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mparent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (child_name, child_module), (child_name_c, child_module_c) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parent_module\u001b[38;5;241m.\u001b[39mnamed_children(), parent_module_c\u001b[38;5;241m.\u001b[39mnamed_children()):\n",
      "\u001b[0;31mAssertionError\u001b[0m: ('up_proj', 'down_proj')"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def foo(parent_name, parent_module, parent_name_c, parent_module_c):\n",
    "    assert parent_name == parent_name_c, (parent_name, parent_name_c)\n",
    "    print(f'\\nparent {parent_name}')\n",
    "    for (child_name, child_module), (child_name_c, child_module_c) in zip(parent_module.named_children(), parent_module_c.named_children()):\n",
    "        print(child_name, child_name_c)\n",
    "    for (child_name, child_module), (child_name_c, child_module_c) in zip(parent_module.named_children(), parent_module_c.named_children()):\n",
    "        foo(child_name, child_module, child_name_c, child_module_c)\n",
    "        \n",
    "foo(\"model\", model.to(\"cuda:1\"), \"model\", custom_model.to(\"cuda:1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(output[0], torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(5, 5, device=\"cuda\")\n",
    "b = torch.randn(5, device=\"cuda\")\n",
    "with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    out = lin(b)\n",
    "    print(out.dtype)\n",
    "    loss = out.norm()\n",
    "    print(loss.dtype)\n",
    "loss.backward()\n",
    "print(lin.weight.grad.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.2246e-02, -2.7771e-03, -4.8340e-02, -9.6680e-02, -7.7637e-02],\n",
       "        [ 5.5237e-03, -2.9182e-04, -5.0964e-03, -1.0193e-02, -8.1787e-03],\n",
       "        [ 2.7148e-01, -1.4404e-02, -2.5195e-01, -5.0391e-01, -4.0430e-01],\n",
       "        [-1.2012e-01,  6.3477e-03,  1.1084e-01,  2.2168e-01,  1.7871e-01],\n",
       "        [-1.9141e-01,  1.0132e-02,  1.7676e-01,  3.5352e-01,  2.8516e-01]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = lin.weight.clone()\n",
    "weight_bf16 = weight.clone().to(torch.bfloat16)\n",
    "weight.eq(weight_bf16.to(torch.float32)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "grad = lin.weight.grad.clone()\n",
    "grad_bf16 = grad.clone().to(torch.bfloat16)\n",
    "grad.eq(grad_bf16.to(torch.float32)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Creates some tensors in default dtype (here assumed to be float32)\n",
    "a_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "b_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "c_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "d_float32 = torch.rand((8, 8), device=\"cuda\")\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "    # torch.mm is on autocast's list of ops that should run in float16.\n",
    "    # Inputs are float32, but the op runs in float16 and produces float16 output.\n",
    "    # No manual casts are required.\n",
    "    e_float16 = torch.mm(a_float32, b_float32)\n",
    "    print(e_float16.dtype)\n",
    "    # Also handles mixed input types\n",
    "    f_float16 = torch.mm(d_float32, e_float16)\n",
    "    print(f_float16.dtype)\n",
    "\n",
    "# After exiting autocast, calls f_float16.float() to use with d_float32\n",
    "g_float32 = torch.mm(d_float32, f_float16.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTensorCustomOptimizer(torch.optim.Optimizer):\n",
    "    def init(self, params, lr=1e-3):\n",
    "        defaults = dict(lr=lr)\n",
    "        super().init(params, defaults)\n",
    "        \n",
    "        # Create global projection matrices\n",
    "        self.global_proj_to_low = torch.randn(20, 100)  # for 100x100 -> 20x100\n",
    "        self.global_proj_to_full = torch.randn(100, 20)  # for 20x100 -> 100x100\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                full_grad = p.grad\n",
    "                state = self.state[p]\n",
    "                grad_proj = torch.mm(state[\"proj\"], full_grad)\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(grad_proj)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(grad_proj)\n",
    "\n",
    "                ###\n",
    "                # Normal AdamW update with grad_proj instead of p.grad to compute exp_avg and denom\n",
    "                ### \n",
    "                update_proj = exp_avg / denom\n",
    "                update = torch.mm(state[\"proj\"].T, update_proj)\n",
    "\n",
    "                p.add_(update, value=-step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"0-fsdp_ckpt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ddp = torch.tensor([ 1.8258e-06, -3.6452e-06,  1.5652e-06,  4.5530e-06,  1.6738e-05,\n",
    "         5.8220e-06, -1.3755e-06, -1.8935e-06])\n",
    "fsdp = torch.tensor([ 2.9776e-06, -5.9446e-06,  2.5525e-06,  7.4251e-06,  2.7296e-05,\n",
    "         9.4945e-06, -2.2432e-06, -3.0879e-06])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6132, 0.6132, 0.6132, 0.6132, 0.6132, 0.6132, 0.6132, 0.6132])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddp / fsdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3163136 // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790784"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "790784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8192128 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096000 * 2 * 4 + 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
